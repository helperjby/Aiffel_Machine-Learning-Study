{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp12. Make chatbot using koean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 프로젝트 목표\n",
    "***\n",
    "* 트랜스포머의 인코더, 디코더 구조를 이해한다.\n",
    "* 내부 단어 토크나이저를 사용하고 셀프 어텐션을 이해한다.\n",
    "* 한국어에 적용하여 챗봇 모델을 구현하고 성능을 체크한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 루브릭\n",
    "***\n",
    "|**평가문항**|**상세기준**|\n",
    "|------------|-------------|\n",
    "|1.  한국어 전처리를 통해 학습 데이터셋을 구축하였다.|공백과 특수문자 처리, 토크나이징, 병렬데이터 구축의 과정이 적절히 진행되었다.|\n",
    "|2. 트랜스포머 모델을 구현하여 한국어 챗봇 모델 학습을 정상적으로 진행하였다.|구현한 트랜스포머 모델이 한국어 병렬 데이터 학습 시 안정적으로 수렴하였다.|\n",
    "|3.  한국어 입력문장에 대해 한국어로 답변하는 함수를 구현하였다.|한국어 입력문장에 그럴듯한 한국어로 답변을 리턴하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 작업환경 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Helpe\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages') # 라이브러리 경로 오류\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'D:/project/aiffel_exp/exp12_transformer/songys_chatbot/ChatBotData.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "questions_part = data['Q']\n",
    "questions = questions_part.values.tolist()\n",
    "print(type(questions))\n",
    "\n",
    "answers_part = data['A']\n",
    "answers = answers_part.values.tolist()\n",
    "print(type(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강아지 키울 수 있을까\n",
      "먼저 생활패턴을 살펴 보세요.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11823 entries, 0 to 11822\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Q       11823 non-null  object\n",
      " 1   A       11823 non-null  object\n",
      " 2   label   11823 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 277.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11823.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.803180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.812012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label\n",
       "count  11823.000000\n",
       "mean       0.803180\n",
       "std        0.812012\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        1.000000\n",
       "75%        2.000000\n",
       "max        2.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.Q[70])\n",
    "print(data.A[70])\n",
    "\n",
    "data.info()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11662\n",
      "7779\n"
     ]
    }
   ],
   "source": [
    "# 중복 데이터 확인\n",
    "print(data['Q'].nunique())  \n",
    "print(data['A'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q        0\n",
       "A        0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 확인\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "정규 표현식(Regular Expression) 을 사용하여 구두점(punctuation) 을 제거하여 \n",
    "단어를 토크나이징(tokenizing) 하는 일에 방해가 되지 않도록 정제하는 것을 목표\n",
    "'''\n",
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = sentence.lower().strip()\n",
    "\n",
    "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "  # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "  # student와 온점 사이에 거리를 만듭니다.\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "  sentence = re.sub(r\"[^a-zA-Zㄱ-ㅣ가-힣0-9?.!,]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = [], []\n",
    "for i in range(len(questions)):\n",
    "     # 전처리 함수를 질문에 해당되는 inputs와 답변에 해당되는 outputs에 적용.\n",
    "    inputs.append(preprocess_sentence(questions[i]))\n",
    "    outputs.append(preprocess_sentence(answers[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장합니다.\n",
    "questions, answers = inputs, outputs\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 22번째 질문 샘플: 어찌해야 하나 .\n",
      "전처리 후의 22번째 답변 샘플: 어찌하면 좋을까요 .\n"
     ]
    }
   ],
   "source": [
    "print('전처리 후의 22번째 질문 샘플: {}'.format(questions[7000]))\n",
    "print('전처리 후의 22번째 답변 샘플: {}'.format(answers[7000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SubwordTextEncoder 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\n",
      "슝=3 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(\"살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\")\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성. (Tensorflow 2.3.0 이상) (클라우드는 2.4 입니다)\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "print(\"슝=3 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(\"슝=3 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8170]\n",
      "END_TOKEN의 번호 : [8171]\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰에 부여된 정수를 출력해봅시다.\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8172\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 단어를 고유한 정수로 인코딩(Integer encoding) & 패딩(Padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5761, 610, 2490, 4163]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2356, 7510, 7, 6273, 97, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 40\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8172\n",
      "필터링 후의 질문 샘플 개수: 11823\n",
      "필터링 후의 답변 샘플 개수: 11823\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "교사 강요(Teacher Forcing) 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 모델 구성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 멀티 헤드 어텐션 구현\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더에서 패딩을 위한 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 패딩 마스킹 구현\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Look-ahead masking\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "print(\"슝=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 인코더 층을 쌓아 인코더 만들기\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "디코더의 층은 임베딩 층(Embedding layer) 과 포지셔널 인코딩(Positional Encoding) 을 \n",
    "연결하고, 사용자가 원하는 만큼 디코더 층을 쌓아 트랜스포머의 디코더가 완성됩니다.\n",
    "인코더와 마찬가지로 num_layers 개수의 디코더 층을 쌓습니다. \n",
    "논문에서는 총 6개의 디코더 층을 사용하였지만, 실습에서는 학습 시간을 고려하여 \n",
    "그보다 적은 개수를 사용할 예정입니다\n",
    "'''\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # 패딩 마스크\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inputs (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " encoder (Functional)           (None, None, 256)    3146240     ['inputs[0][0]',                 \n",
      "                                                                  'enc_padding_mask[0][0]']       \n",
      "                                                                                                  \n",
      " look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n",
      "                                e)                                                                \n",
      "                                                                                                  \n",
      " dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, None, 256)    3673600     ['dec_inputs[0][0]',             \n",
      "                                                                  'encoder[0][0]',                \n",
      "                                                                  'look_ahead_mask[0][0]',        \n",
      "                                                                  'dec_padding_mask[0][0]']       \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, None, 8172)   2100204     ['decoder[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,920,044\n",
      "Trainable params: 8,920,044\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 손실함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Learing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "185/185 [==============================] - 18s 67ms/step - loss: 1.4475 - accuracy: 0.0311\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 1.1749 - accuracy: 0.0495\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 13s 69ms/step - loss: 1.0039 - accuracy: 0.0508\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 13s 71ms/step - loss: 0.9273 - accuracy: 0.0545\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.8690 - accuracy: 0.0577\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 13s 73ms/step - loss: 0.8101 - accuracy: 0.0618\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 15s 83ms/step - loss: 0.7455 - accuracy: 0.0677\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 14s 76ms/step - loss: 0.6738 - accuracy: 0.0754\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 12s 67ms/step - loss: 0.5964 - accuracy: 0.08362s - loss: 0.5998 - accuracy: 0.08 - ETA: 2s -\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 12s 67ms/step - loss: 0.5137 - accuracy: 0.0931\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.4312 - accuracy: 0.10330s - loss: 0.4312 - accuracy: 0.10\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.3498 - accuracy: 0.1145\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 12s 66ms/step - loss: 0.2746 - accuracy: 0.1253\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 12s 64ms/step - loss: 0.2087 - accuracy: 0.1356\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 12s 65ms/step - loss: 0.1541 - accuracy: 0.14480s - loss: 0.1535 - accura - ETA: 0s - loss: 0.1\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 11s 61ms/step - loss: 0.1109 - accuracy: 0.1528\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 12s 64ms/step - loss: 0.0804 - accuracy: 0.15830s - loss: 0.0792 - accuracy: 0. - ETA: 0s - loss: 0.079\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 12s 67ms/step - loss: 0.0614 - accuracy: 0.1617\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 14s 76ms/step - loss: 0.0517 - accuracy: 0.1633\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 12s 67ms/step - loss: 0.0448 - accuracy: 0.1647\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - 12s 65ms/step - loss: 0.0423 - accuracy: 0.16480s - loss:\n",
      "Epoch 22/50\n",
      "185/185 [==============================] - 11s 61ms/step - loss: 0.0417 - accuracy: 0.1647\n",
      "Epoch 23/50\n",
      "185/185 [==============================] - 12s 63ms/step - loss: 0.0362 - accuracy: 0.16600s -\n",
      "Epoch 24/50\n",
      "185/185 [==============================] - 12s 66ms/step - loss: 0.0318 - accuracy: 0.16710s - loss: 0.0312 - accu - ETA: 0s - loss: 0.031\n",
      "Epoch 25/50\n",
      "185/185 [==============================] - 11s 62ms/step - loss: 0.0276 - accuracy: 0.1683\n",
      "Epoch 26/50\n",
      "185/185 [==============================] - 12s 63ms/step - loss: 0.0248 - accuracy: 0.1688\n",
      "Epoch 27/50\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0214 - accuracy: 0.1697\n",
      "Epoch 28/50\n",
      "185/185 [==============================] - 12s 66ms/step - loss: 0.0200 - accuracy: 0.1701\n",
      "Epoch 29/50\n",
      "185/185 [==============================] - 12s 65ms/step - loss: 0.0185 - accuracy: 0.1704\n",
      "Epoch 30/50\n",
      "185/185 [==============================] - 12s 64ms/step - loss: 0.0164 - accuracy: 0.1710\n",
      "Epoch 31/50\n",
      "185/185 [==============================] - 12s 67ms/step - loss: 0.0152 - accuracy: 0.1713\n",
      "Epoch 32/50\n",
      "185/185 [==============================] - 12s 65ms/step - loss: 0.0144 - accuracy: 0.1715\n",
      "Epoch 33/50\n",
      "185/185 [==============================] - 12s 66ms/step - loss: 0.0127 - accuracy: 0.1720\n",
      "Epoch 34/50\n",
      "185/185 [==============================] - 13s 71ms/step - loss: 0.0121 - accuracy: 0.17200s - loss: 0.0\n",
      "Epoch 35/50\n",
      "185/185 [==============================] - 11s 62ms/step - loss: 0.0114 - accuracy: 0.17240s - loss:\n",
      "Epoch 36/50\n",
      "185/185 [==============================] - 12s 65ms/step - loss: 0.0106 - accuracy: 0.1725\n",
      "Epoch 37/50\n",
      "185/185 [==============================] - 12s 67ms/step - loss: 0.0101 - accuracy: 0.1725\n",
      "Epoch 38/50\n",
      "185/185 [==============================] - 12s 63ms/step - loss: 0.0096 - accuracy: 0.1728\n",
      "Epoch 39/50\n",
      "185/185 [==============================] - 12s 64ms/step - loss: 0.0089 - accuracy: 0.17290s - loss: 0.0089 - accura\n",
      "Epoch 40/50\n",
      "185/185 [==============================] - 12s 63ms/step - loss: 0.0087 - accuracy: 0.1729\n",
      "Epoch 41/50\n",
      "185/185 [==============================] - 12s 62ms/step - loss: 0.0086 - accuracy: 0.1730\n",
      "Epoch 42/50\n",
      "185/185 [==============================] - 12s 63ms/step - loss: 0.0072 - accuracy: 0.1733\n",
      "Epoch 43/50\n",
      "185/185 [==============================] - 12s 64ms/step - loss: 0.0080 - accuracy: 0.1731\n",
      "Epoch 44/50\n",
      "185/185 [==============================] - 11s 62ms/step - loss: 0.0072 - accuracy: 0.1734\n",
      "Epoch 45/50\n",
      "185/185 [==============================] - 12s 62ms/step - loss: 0.0069 - accuracy: 0.17340s - loss: 0.0069 - accuracy: 0.17 - ETA: 0s - loss: 0.0069 - accuracy\n",
      "Epoch 46/50\n",
      "185/185 [==============================] - 12s 64ms/step - loss: 0.0067 - accuracy: 0.1734\n",
      "Epoch 47/50\n",
      "185/185 [==============================] - 12s 63ms/step - loss: 0.0062 - accuracy: 0.1735\n",
      "Epoch 48/50\n",
      "185/185 [==============================] - 12s 62ms/step - loss: 0.0062 - accuracy: 0.1735\n",
      "Epoch 49/50\n",
      "185/185 [==============================] - 12s 64ms/step - loss: 0.0059 - accuracy: 0.1737\n",
      "Epoch 50/50\n",
      "185/185 [==============================] - 11s 62ms/step - loss: 0.0055 - accuracy: 0.17370s - loss: 0.0054  - ETA: 0s - loss: 0.0055 - accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa0bf2d340>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 예측 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 대답 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def sentence_generation(sentence):\n",
    "      # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('입력 : {}'.format(sentence))\n",
    "  print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 챗봇 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 이렇게 하는게 맞는걸까\n",
      "출력 : 대부분은 먼저 연락하곤 해요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'대부분은 먼저 연락하곤 해요 .'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('이렇게 하는게 맞는걸까')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 시간이 너무 빠르다\n",
      "출력 : 시간은 상대적이죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'시간은 상대적이죠 .'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('시간이 너무 빠르다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 힘내세요\n",
      "출력 : 항상 못해본 건 궁금하더라고요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'항상 못해본 건 궁금하더라고요 .'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('힘내세요')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 느낀점\n",
    "***\n",
    "* 데이터 전처리 과정에서 중복 문장을 제거하는게 맞는 것인지, 어떤 차이가 있는지 궁금점이 생겼다.\n",
    "* 이전에 진행했던 mecab을 이용한 토크나이저 생성과 어떤 차이가 있는지도 궁금함이 생겼다.\n",
    "* 관련 내용을 깊게 이해하지 못하고 코드만 나열한 과제였다. 꼭 다시 들여다봐야겠다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a79ad638c2a33e4c200ee9d03a630086321746eff3c6a16fb4371b088891321"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
