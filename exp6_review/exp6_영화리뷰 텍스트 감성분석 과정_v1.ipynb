{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp 6.영화리뷰 텍스트 감성분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 목표\n",
    "***\n",
    "* 텍스트 데이터를 머신러닝 입출력용 수치데이터로 변환하는 과정을 이해한다.\n",
    "* RNN의 특징을 이해하고 시퀀셜한 데이터를 다루는 방법을 이해한다.\n",
    "* 1-D CNN으로도 텍스트를 처리할 수 있음을 이해한다.\n",
    "* IMDB와 네이버 영화리뷰 데이터셋을 이용한 영화리뷰 감성 분류 실습을 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-2. 텍스트 감정분석의 유용성\n",
    "\n",
    "딥러닝을 통해 IMDb나 네이버 영화 리뷰 텍스트에 담긴 이용자의 감성이 긍정적인지 부정적인지를 분류하는 모델 만들기\n",
    "\n",
    "딥러닝을 이용한 텍스트 감성분석은 어떤 점에서 유용한가?\n",
    "\n",
    "* 텍스트 데이터만이 가지고 있는 정보적 특성과 가치는 어떤 것일까요?\n",
    "* 감성분석 등 텍스트 분류 모델이 다른 데이터 분석 업무에 어떤 점에서 도움을 주나요?\n",
    "* 텍스트 데이터 분석의 기술적 어려움은 무엇인가요?\n",
    "* 텍스트 분류 작업을 하는데 딥러닝이 적용되면 어떤 점에서 유리해질까요?\n",
    "\n",
    "위 질문들에 대한 답을 제공하는 아티클\n",
    "동아비즈니스리뷰 감성분석 활용 사례 기고\n",
    "https://dbr.donga.com/article/view/1202/article_no/8891/ac/magazine\n",
    "\n",
    "```\n",
    "Q1. 텍스트 데이터에서만 얻을 수 있는 유용한 정보는 무엇인가요? 그 유용성은 텍스트 데이터의 어떤 특징으로부터 비롯되는 것인가요?  \n",
    "A1. SNS 등에서 광범위한 분량의 텍스트 데이터를 쉽게 얻을 수 있는데, 이 데이터는 소비자들의 개인적, 감성적 반응이 직접 담겨 있을뿐더러 실시간 트렌드를 빠르게 반영하는 데이터이기도 하다.\n",
    "```\n",
    "```\n",
    "Q2. 텍스트 감성분석 접근법을 크게 2가지로 나누면 무엇과 무엇이 있나요?  \n",
    "A2. 기계학습 기반 접근법과 감성사전 기반 접근법\n",
    "```\n",
    "```\n",
    "Q3. 사전 기반의 감성분석이 기계학습 기반 접근법 대비 가지는 한계점을 2가지만 들어 주세요.  \n",
    "A3. \n",
    "1.분석 대상에 따라 단어의 감성 점수가 달라질 수 있다는 가능성에 대응하기 어렵다.\n",
    "2.단순 긍부정을 넘어서 긍부정의 원인이 되는 대상 속성 기반의 감성 분석이 어렵다.\n",
    "```\n",
    "```\n",
    "Q4. 감성분석 등 텍스트 분류 모델이 다른 데이터분석 업무에 어떤 도움을 줄 수 있나요?\n",
    "A4. 일반적인 데이터분석 업무는 범주화가 잘 된 정형데이터를 필요로 하는데, 이런 데이터를 큰 규모로 구축하기 위해서 많은 비용이 들지만, 쉽게 구할 수 있는 비정형데이터인 텍스트에 감성분석 기법을 적용하면 텍스트를 정형데이터로 가공하여 유용한 의사결정 보조자료로 활용할 수 있게 된다.\n",
    "```\n",
    "```\n",
    "Q5. 라벨링 비용이 많이 드는 머신러닝 기반 감성분석의 비용을 절감하면서 정확도를 크게 향상시킬 수 있는 자연어처리 기법에는 무엇이 있나요?\n",
    "A5. 단어의 특성을 저차원 벡터값으로 표현할 수 있는 워드 임베딩(word embedding) 기법\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 텍스트 데이터의 특징\n",
    "\n",
    "텍스트를 입력받아서 1, 0으로 이진 분류하는 모델.. \n",
    "* 텍스트를 어떻게 숫자 행렬로 표현할 수 있나요?\n",
    "* 텍스트에는 순서가 중요합니다. 입력 데이터의 순서를 인공지능 모델에 어떻게 반영해야 하나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 텍스트를 숫자로 표현하는 방법\n",
    "\n",
    "인공지능 모델은 0과 1의 비트로 표현 가능한 숫자로 이루어진 매트릭스만 입력 가능  \n",
    "만약 A=0, B=1, ... Z=25라고 숫자를 부여해도 의미적으로 A와 Z는 25만큼 멀지 않다  \n",
    "-> 텍스트의 중요한 특징. 그 자체로는 기호일 뿐이며 텍스트가 가진 의미를 기호가 내포하지 않음\n",
    "\n",
    "그렇다면 어떻게?  \n",
    "단어와 그 단어의 의미를 나타내는 벡터를 짝지음 -> 단어 사전 제작  \n",
    "딥러닝을 통해 벡터를 확보 가능함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'feel', 'hungry']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "i feel hungry\n",
    "i eat lunch\n",
    "now i feel happy\n",
    "'''\n",
    "\n",
    "# 처리해야 할 문장을 파이썬 리스트에 옮겨 담았습니다.\n",
    "sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# 파이썬 split() 메소드를 이용해 단어 단위로 문장을 쪼개 봅니다.\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 데이터의 모든 문장을 단어 단위로 쪼갠 후 딕셔너리 자료구조로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"
     ]
    }
   ],
   "source": [
    "index_to_word={}  # 빈 딕셔너리를 만들어서\n",
    "\n",
    "# 단어들을 하나씩 채워 봅니다. 채우는 순서는 일단 임의로 하였습니다. 그러나 사실 순서는 중요하지 않습니다. \n",
    "# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다. \n",
    "index_to_word[0]='<PAD>'  # 패딩용 단어\n",
    "index_to_word[1]='<BOS>'  # 문장의 시작지점\n",
    "index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\n",
    "index_to_word[3]='i'\n",
    "index_to_word[4]='feel'\n",
    "index_to_word[5]='hungry'\n",
    "index_to_word[6]='eat'\n",
    "index_to_word[7]='lunch'\n",
    "index_to_word[8]='now'\n",
    "index_to_word[9]='happy'\n",
    "\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 데이터를 숫자로 바꾸려면 딕셔너리의 구조가 {텍스트:인덱스} 구조여야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'i': 3, 'feel': 4, 'hungry': 5, 'eat': 6, 'lunch': 7, 'now': 8, 'happy': 9}\n"
     ]
    }
   ],
   "source": [
    "# 인덱싱\n",
    "word_to_index={word:index for index, word in index_to_word.items()}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index['feel'])  # 단어 'feel'은 숫자 인덱스 4로 바뀝니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 데이터를 숫자로 바꿔 표현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_encoded_sentence` 함수를 통해 매핑됨\n",
    "\n",
    "* `<BOS>` -> 1\n",
    "* i -> 3\n",
    "* eat -> 6\n",
    "* lunch -> 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다. \n",
    "encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode된 벡터를 decode하여 원래 텍스트로 복구도 가능하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i feel hungry\n"
     ]
    }
   ],
   "source": [
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i feel hungry', 'i eat lunch', 'now i feel happy']\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 가 아래와 같이 변환됩니다.\n",
    "print(get_decoded_sentences(encoded_sentences, index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Embedding 레이어의 등장\n",
    "\n",
    "텍스트를 숫자로 변환하여 인공지능 모델에 입력할 수 있지만 , 이 벡터는 텍스트에 담긴 언어의 의미와 대응하지 않음 \n",
    " \n",
    "우리의 목표는 단어와 그 단어의 의미를 나타내는 벡터를 짝짓는 것   \n",
    "단어의 의미를 나타내는 벡터를 훈련 가능한 파라미터로 놓고 이를 딥러닝을 통해 학습해서 최적화함\n",
    "\n",
    "이 과정이 가능하도록 구현한게 Tensorflow, Pytorch 등의 딥러닝 프레임워크들의 `Embedding 레이어`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에러가 나는 이유\n",
    "\n",
    "Eembedding 레이어의 인풋 문장 벡터는 그**길이가 일정**해야 함.  \n",
    "raw_inputs의 3개 벡터의 길이는 4,4,5\n",
    "\n",
    "이러한 에러를 방지하기 위해 Tensorflow의 `tf.keras.preprocessing.sequence.pad_sequences` 함수 사용  \n",
    "문장 벡터 뒤에 패딩(`<PAD>`)을 추가하여 길이를 일정하게 맞춰준다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4880/950946431.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n\u001b[0m\u001b[0;32m      2\u001b[0m                                                        \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<PAD>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                                        \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pre'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                        maxlen=5)\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=5)\n",
    "print(raw_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "짧은 문장의 뒤에 0이 채워지는 걸 확인 가능함 저 0이 매핑된 `<PAD>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.0415719   0.04510773 -0.03393795 -0.04314945]\n",
      "  [-0.00201686 -0.02497222 -0.00084921 -0.01527202]\n",
      "  [-0.03646073  0.0038466  -0.02191924 -0.0152797 ]\n",
      "  [-0.02817558  0.0225426  -0.01946044  0.01952989]\n",
      "  [ 0.02465181  0.02052424 -0.04752368 -0.03355368]]\n",
      "\n",
      " [[-0.0415719   0.04510773 -0.03393795 -0.04314945]\n",
      "  [-0.00201686 -0.02497222 -0.00084921 -0.01527202]\n",
      "  [-0.02550598  0.00647353 -0.01069086  0.02726653]\n",
      "  [-0.03470414 -0.01356866 -0.01811626 -0.01386415]\n",
      "  [ 0.02465181  0.02052424 -0.04752368 -0.03355368]]\n",
      "\n",
      " [[-0.0415719   0.04510773 -0.03393795 -0.04314945]\n",
      "  [-0.03726051 -0.04351171  0.03726638  0.01566175]\n",
      "  [-0.00201686 -0.02497222 -0.00084921 -0.01527202]\n",
      "  [-0.03646073  0.0038466  -0.02191924 -0.0152797 ]\n",
      "  [ 0.03247916  0.04855027 -0.04672369  0.03219018]]], shape=(3, 5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 그림과 같이 4차원의 워드 벡터를 가정합니다.\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# tf.keras.preprocessing.sequence.pad_sequences를 통해 word vector를 모두 일정 길이로 맞춰주어야 \n",
    "# embedding 레이어의 input이 될 수 있음에 주의해 주세요. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype=object)\n",
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q6. output의 shape=(3, 5, 4)에서 3, 5, 4의 의미는 각각 무엇일까요?  \n",
    "A6. 3은 입력문장 개수, 5는 입력문장의 최대 길이, 4는 워드 벡터의 차원 수\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 시퀀스 데이터를 다루는 RNN\n",
    "\n",
    "텍스트 데이터를 다룰 때 주로 사용하는 딥러닝 모델 `Recurrent Neural Network(RNN)`  \n",
    "RNN은 시퀀스(Sequence) 형태의 데이터를 처리하는데 최적  \n",
    "-> 시퀀스 데이터란 음성 데이터처럼 입력이 시간 축을 따라 발생하는 데이터  \n",
    "-> RNN은 시간의 흐름에 따라 새롭게 들어오는 입력을 반영하여 현재 상태를 묘사하는 state machine으로 설계됨  \n",
    "\n",
    "\n",
    "**State machine**?\n",
    "\n",
    "![state 설명그림](https://d3s0tskafalll9.cloudfront.net/media/images/E-9-3.max-800x600.png)\n",
    "\n",
    "```\n",
    "Q7. 위 그림에서 대화가 stateful한지 stateless한지 결정하는 것은 직원인가요, 아니면 손님인가요? 그렇게 생각하는 이유는 무엇인가요?  \n",
    "A7. Stateful한 대화에서는 손님이 이전 시점에 어떤 선택을 했는지 직원이 기억을 하지만, Stateless한 대화에서는 직원이 기억하지 못한다. 그래서 손님 스스로 본인이 이전 시점에 했던 선택을 모두 기억하고 있다가 직원에게 매번 새롭게 전달해야 한다. 손님의 이전 주문내역을 기억하는 직원은 stateful하고, 그렇지 못한 직원은 stateless하다.\n",
    "```\n",
    "\n",
    "김성훈 교수의 모두의 딥러닝 강좌 12강.RNN  \n",
    "https://youtu.be/-SHPG_KMUkQ\n",
    "\n",
    "```\n",
    "Q8. RNN의 정의대로라면 t=4 시점의 state h4는 t=4 시점의 input x4와 t=3 시점의 state h3가 결정합니다. 그렇다면 h4에는 t<4 이전의 입력 x1, x2, x3의 정보는 반영되지 않는 것일까요?\n",
    "A8. 그렇지 않습니다. h4를 결정하는 이전 state h3 안에 x3의 정보가 반영되어 있고, 같은 원리로 이전 시점의 모든 입력의 정보가 현재 상태에 반영될 수 있습니다.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN 모델을 사용하여 텍스트 데이터를 처리하는 예제 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, None, 4)           40        \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 8)                 416       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 537\n",
      "Trainable params: 537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 RNN 외의 다른 텍스트 처리 모델들\n",
    "\n",
    "`1-D Convolution Neural Network(1-D CNN)`  \n",
    "`1-D CNN`은 문장 전체를 한번에 한 방향으로 길이 7짜리 필터로 스캐닝하면서 7단어 이내에서 발견되는 특징을 추출하고 그것으로 문장을 분석함  \n",
    "CNN 계열도 RNN 못지않은 효율을 보여주며 RNN 계열보다 병렬처리가 효율적이라 학습 속도가 빠르다는 장점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, None, 4)           40        \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 16)          464       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, None, 16)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 16)          1808      \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 16)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,457\n",
      "Trainable params: 2,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GlobalMaxPooling1D()` 레이어 하나만 사용하는 방법?  \n",
    "전체 문장 중에서 단 하나의 가장 중요한 단어만 피처로 추출 하여 그것으로 문장의 긍정/부정을 평가하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, None, 4)           40        \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 4)                0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 40        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89\n",
      "Trainable params: 89\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 외에도 1-D CNN과 RNN 레이어를 섞어 쓴다거나  \n",
    "FFN(FeedForward Network) 레이어만으로 구성하거나  \n",
    "최근 각광받고 있는 Transformer 레이어를 쓰는 등 다양한 시도 가능\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 IMDB 데이터셋 분석\n",
    "\n",
    "IMDb Large Movie Dataset은  \n",
    "50000개의 영어로 작성된 영화 리뷰 텍스트로 구성되어 있으며, 긍정은 1, 부정은 0의 라벨이 달려 있음\n",
    "\n",
    "2011년 Learning Word Vectors for Sentiment Analysis 논문에서 소개됨\n",
    "https://aclanthology.org/P11-1015.pdf\n",
    "\n",
    "50000개 중 25000개가 훈련용 데이터, 나머지 25000개를 테스트 데이터로 사용하도록 지정되어 있음  \n",
    "데이터는 tensorflow Keras 데이터셋 안에 포함되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 개수: 25000, 테스트 개수: 25000\n"
     ]
    }
   ],
   "source": [
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "# IMDb 데이터셋 다운로드 \n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(x_train), len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`imdb.load_data()` 호출 시 단어사전에 등재할 단어의 개수(`num_words`)를 10000으로 지정하면  \n",
    "그 개수만큼의 `word_to_index `딕셔너리까지 생성된 형태로 데이터셋이 생성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "라벨:  1\n",
      "1번째 리뷰 문장 길이:  218\n",
      "2번째 리뷰 문장 길이:  189\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])  # 1번째 리뷰데이터\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n",
    "print('1번째 리뷰 문장 길이: ', len(x_train[0]))\n",
    "print('2번째 리뷰 문장 길이: ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트가 아닌 이미 숫자로 encode된 데이터  \n",
    "이미 텍스트가 encode되었으므로 IMDb 데이터셋에서는 encode에 사용한 딕셔너리까지 제공한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])     # 'the' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 1 이 출력됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDb 데이터셋의 텍스트 인코딩을 위한 `word_to_index`, `index_to_word`는 아래와 같이 보정되어야 함  \n",
    "아래 내용은 Tensorflow 튜토리얼의 가이드를 반영하여 작성됨  \n",
    "`word_to_index`는 IMDb 텍스트 데이터셋의 단어 출현 빈도 기준으로 내림차수 정렬되어 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\n",
      "4\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 4 이 출력됩니다. \n",
    "print(index_to_word[4])     # 'the' 가 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "라벨:  1\n"
     ]
    }
   ],
   "source": [
    "# encode된 텍스트가 정상적으로 decode되는지 확인\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decode한 문장과 라벨을 비교하여 일치하는지 확인 -> 이 부분 모르겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pad_sequences`를 통해 데이터셋 상의 문장의 길이를 통일해야 함  \n",
    "`maxlen`(문장 최대 길이)의 적절한 값을 찾기 위해서는 전체 데이터셋의 분포를 확인해봐야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  234.75892\n",
      "문장길이 최대 :  2494\n",
      "문장길이 표준편차 :  172.91149458735703\n",
      "pad_sequences maxlen :  580\n",
      "전체 문장의 0.94536%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 경우 `maxlen=580`  \n",
    "padding 방식(문장 뒤쪽('post')와 앞쪽('pre'))에 따라 딥러닝 적용 시 성능 차이가 발생  \n",
    "-> 두 가지 방식을 한 번씩 적용하여 RNN 학습 후 결과 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 580)\n"
     ]
    }
   ],
   "source": [
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'post'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 혹은 'post'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q9. RNN 활용 시 pad_sequences의 padding 방식은 'post'와 'pre' 중 어느 것이 유리할까요? 그 이유는 무엇일까요?\n",
    "A9. RNN은 입력데이터가 순차적으로 처리되어, 가장 마지막 입력이 최종 state 값에 가장 영향을 많이 미치게 됩니다. 그러므로 마지막 입력이 무의미한 padding으로 채워지는 것은 비효율적입니다. 따라서 'pre'가 훨씬 유리하며, 10% 이상의 테스트 성능 차이를 보이게 됩니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 딥러닝 모델 설계와 훈련\n",
    "\n",
    "RNN 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, None, 16)          160000    \n",
      "                                                                 \n",
      " global_max_pooling1d_3 (Glo  (None, 16)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,145\n",
      "Trainable params: 160,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data 25000개 중 10000개를 validation 으로 분리  \n",
    "적절한 train과 validation 비율은 무엇인지 고민해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 580)\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 10000건 분리\n",
    "x_val = x_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = x_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 15s 198ms/step - loss: 0.6905 - accuracy: 0.5899 - val_loss: 0.6861 - val_accuracy: 0.6048\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 5s 170ms/step - loss: 0.6796 - accuracy: 0.6275 - val_loss: 0.6718 - val_accuracy: 0.6510\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 4s 141ms/step - loss: 0.6583 - accuracy: 0.7200 - val_loss: 0.6445 - val_accuracy: 0.7591\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 4s 144ms/step - loss: 0.6210 - accuracy: 0.7863 - val_loss: 0.6038 - val_accuracy: 0.7862\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.5713 - accuracy: 0.8206 - val_loss: 0.5559 - val_accuracy: 0.8067\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.5143 - accuracy: 0.8402 - val_loss: 0.5063 - val_accuracy: 0.8208\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 0.4569 - accuracy: 0.8578 - val_loss: 0.4610 - val_accuracy: 0.8305\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 0.4042 - accuracy: 0.8701 - val_loss: 0.4241 - val_accuracy: 0.8379\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.3591 - accuracy: 0.8813 - val_loss: 0.3961 - val_accuracy: 0.8437\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 0.3215 - accuracy: 0.8915 - val_loss: 0.3753 - val_accuracy: 0.8475\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 1s 42ms/step - loss: 0.2899 - accuracy: 0.9023 - val_loss: 0.3612 - val_accuracy: 0.8508\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.2639 - accuracy: 0.9105 - val_loss: 0.3509 - val_accuracy: 0.8550\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.2414 - accuracy: 0.9182 - val_loss: 0.3441 - val_accuracy: 0.8571\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.2219 - accuracy: 0.9263 - val_loss: 0.3401 - val_accuracy: 0.8569\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.2045 - accuracy: 0.9336 - val_loss: 0.3379 - val_accuracy: 0.8582\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.1889 - accuracy: 0.9394 - val_loss: 0.3373 - val_accuracy: 0.8575\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1746 - accuracy: 0.9448 - val_loss: 0.3379 - val_accuracy: 0.8573\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.1614 - accuracy: 0.9513 - val_loss: 0.3393 - val_accuracy: 0.8580\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1495 - accuracy: 0.9565 - val_loss: 0.3420 - val_accuracy: 0.8578\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 0.1383 - accuracy: 0.9605 - val_loss: 0.3452 - val_accuracy: 0.8566\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 9s - loss: 0.3640 - accuracy: 0.8444 - 9s/epoch - 11ms/step\n",
      "[0.3639642298221588, 0.8443599939346313]\n"
     ]
    }
   ],
   "source": [
    "# 학습 후 테스트셋으로 모델 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.fit()` 과정 중의 train/validation loss, accuracy 등이 매 epoch마다 history 변수에 저장되어 있습니다.  \n",
    "\n",
    "이 데이터를 그래프로 그려 보면, 수행했던 딥러닝 학습이 잘 진행되었는지, 오버피팅 혹은 언더피팅하지 않았는지, 성능을 개선할 수 있는 다양한 아이디어를 얻을 수 있는 좋은 자료가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation loss 그래프를 통해 적정 epoch 단계를 추정 가능하다  \n",
    "validation loss와 train loss가 이격이 발생하면 그 이상의 훈련은 무의미하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation accuracy 역시 유사한 인사이트를 얻을 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 Word2Vec의 적용\n",
    "\n",
    "`워드 임베딩(word embedding)`  \n",
    "단어의 특성을 저차원 벡터값으로 표현 가능한 기법  \n",
    "감성분석 방식의 라벨링 비용을 절감하면서 정확도를 크게 향상시킬 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = 'D:/project/sentiment_classification/data/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w', encoding='utf-8') # 인코딩 에러가 발생하여 encoding='utf-8' 옵션 추가\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'cp949' codec can't encode ... ` 어쩌고 encoding error  \n",
    "파이썬의 기본 설정이 utf-8이 아니라서 비-영어권 사용자들은 코딩할때 별도의 옵션을 설정해야 오류가 안난다고 한다.  \n",
    "해결방법  \n",
    "`file=open('파일명','w',encoding='UTF-8')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gensim` 에서 제공하는 패키지를 이용해, 위에 남긴 임베딩 파라미터를 읽어서 word vector로 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01754055, -0.04434309, -0.01187165, -0.02456536, -0.04463724,\n",
       "       -0.02891786, -0.01147646, -0.02144857, -0.01918643, -0.00886441,\n",
       "       -0.02751042, -0.02217978, -0.02411162, -0.03451467, -0.0260361 ,\n",
       "       -0.02103702], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "워드 벡터가 의미 벡터 공간상에 유의미하게 학습됐는지 확인하는 방법  \n",
    "`gensim`을 활용해 단어를 하나 주고 그와 가장 유사한 단어와 그 유사도를 확인하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('minor', 0.9858551025390625),\n",
       " ('moving', 0.9823207259178162),\n",
       " ('gives', 0.9814017415046692),\n",
       " ('premiered', 0.9694541692733765),\n",
       " ('explosions', 0.9666252732276917),\n",
       " ('occurs', 0.966166079044342),\n",
       " ('wide', 0.963441789150238),\n",
       " ('strong', 0.96340411901474),\n",
       " ('span', 0.9590501189231873),\n",
       " ('interpretation', 0.9583466649055481)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 다룬 정도의 훈련 데이터로는 워드 벡터를 정교하게 학습시키기 어려움  \n",
    "\n",
    "구글에서 제공하는 `Word2Vec` 이라는 사전학습된(Pretrained) 워드 임베딩 모델 사용  \n",
    "`Word2Vec`?\n",
    "* 1억 개의 단어로 구성된 Google News dataset을 바탕으로 학습\n",
    "* 총 300만 개의 단어를 각각 300차원의 벡터로 표현한 것\n",
    "\n",
    "사전학습된 임베딩을 활용하는 것은 `전이학습`으로 인해 아주 유리하다. \n",
    "\n",
    "```\n",
    "Q10. 사전에 학습된 Word2Vec 등의 임베딩 모델을 활용하는 전이학습(Transfer Learning)이 유리한 이유를 설명해 보세요.\n",
    "A10. 사람도 무언가를 배우기 위해 제로베이스에서 시작하지 않고 자신이 지닌 이전의 경험과 지식을 동원하는 것처럼, 광범위한 데이터를 통해 미리 학습해 놓은 임베딩 속에 녹아 있는 의미, 문법 등의 부가적인 정보를 내가 만들려는 모델이 활용할 수 있는 피처로 활용하는 것이 훨씬 빠르고 정확하게 학습할 수 있는 방법이 된다.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_path = 'D:/project/sentiment_classification/data/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=None)\n",
    "vector = word2vec['computer']\n",
    "vector     # 무려 300dim의 워드 벡터입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300만개의 단어를 모두 메모리에 로딩하면 메모리 에러가 발생할 확률이 높음  \n",
    "``KeyedVectors.load_word2vec_format`` 메서드로 워드 벡터를 로딩할 때 가장 많이 사용되는 상위 100만 개만 limt으로 조건을 주어 로딩  \n",
    "-> 메모리가 충분하다면 `limt=None`으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loved', 0.6907791495323181),\n",
       " ('adore', 0.6816873550415039),\n",
       " ('loves', 0.661863386631012),\n",
       " ('passion', 0.6100708842277527),\n",
       " ('hate', 0.6003956198692322),\n",
       " ('loving', 0.5886635780334473),\n",
       " ('Ilove', 0.5702950954437256),\n",
       " ('affection', 0.5664337873458862),\n",
       " ('undying_love', 0.5547305345535278),\n",
       " ('absolutely_adore', 0.5536840558052063)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메모리를 다소 많이 소비하는 작업이니 유의해 주세요.\n",
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전보다 의미적 유사도가 가까움.  \n",
    "이전 학습에서 사용한 모델의 임베딩 레이어를 `Word2Vec`으로 교체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드 벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 580, 300)          9000000   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 574, 16)           33616     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 114, 16)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 108, 16)           1808      \n",
      "                                                                 \n",
      " global_max_pooling1d_4 (Glo  (None, 16)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,035,569\n",
      "Trainable params: 9,035,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 30000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원 수 \n",
    "\n",
    "# 모델 구성\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 27s 593ms/step - loss: 0.6911 - accuracy: 0.5261 - val_loss: 0.6837 - val_accuracy: 0.5666\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 17s 569ms/step - loss: 0.6743 - accuracy: 0.5861 - val_loss: 0.6586 - val_accuracy: 0.6176\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 14s 460ms/step - loss: 0.6169 - accuracy: 0.6776 - val_loss: 0.5872 - val_accuracy: 0.7037\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 13s 423ms/step - loss: 0.4686 - accuracy: 0.8007 - val_loss: 0.4139 - val_accuracy: 0.8190\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 16s 525ms/step - loss: 0.3157 - accuracy: 0.8737 - val_loss: 0.3470 - val_accuracy: 0.8468\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 18s 590ms/step - loss: 0.2243 - accuracy: 0.9171 - val_loss: 0.3257 - val_accuracy: 0.8626\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 16s 547ms/step - loss: 0.1628 - accuracy: 0.9452 - val_loss: 0.3256 - val_accuracy: 0.8651\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 15s 519ms/step - loss: 0.1147 - accuracy: 0.9683 - val_loss: 0.3363 - val_accuracy: 0.8660\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 15s 496ms/step - loss: 0.0810 - accuracy: 0.9829 - val_loss: 0.3905 - val_accuracy: 0.8561\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 11s 353ms/step - loss: 0.0588 - accuracy: 0.9909 - val_loss: 0.3714 - val_accuracy: 0.8669\n"
     ]
    }
   ],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9 프로젝트 : 네이버 영화리뷰 감성분석\n",
    "\n",
    "네이버 영화의 댓글을 모아 구성된 Naver sentiment movie corpus\n",
    "https://github.com/e9t/nsmc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 루브릭\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 준비와 확인\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Helpe\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages')\n",
    "\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('D:/project/sentiment_classification/data/nsmc-master/ratings_train.txt')\n",
    "test_data = pd.read_table('D:/project/sentiment_classification/data/nsmc-master/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터로더 구성\n",
    "***\n",
    "nsmc 데이터셋은 IMDB에서 제공했던 숫자 인덱스로 변환된 텍스트와 `word_to_index` 딕셔너리가 없다.  \n",
    "가공되지 않은 텍스트 파일들을 `imdb.data_loader()`와 동일하게 동작하는 자신만의 `data_loader`를 만들어 보는 것으로 시작함\n",
    "\n",
    "* 데이터의 중복 제거\n",
    "* NaN 결측치 제거\n",
    "* 한국어 토크나이저로 토큰화\n",
    "* 불용어(Stopwords) 제거\n",
    "* 사전word_to_index 구성\n",
    "* 텍스트 스트링을 사전 인덱스 스트링으로 변환\n",
    "* X_train, y_train, X_test, y_test, word_to_index 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from eunjeon import Mecab #\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "    \n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "    \n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['', '', '', ''] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "        \n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index[''] for word in wordlist]\n",
    "        \n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "        \n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "    \n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Install MeCab in order to use it: http://konlpy.org/en/latest/install/` 오류발생\n",
    "\n",
    "페이지에 들어가니 Warning: KoNLPy’s Mecab() class is not supported on Windows machines. 문구..\n",
    "\n",
    "* 자바 jdk 설치\n",
    "* 시스템에서 JAVA_HOME 설정 \n",
    "* PATH에서 자바 경로 추가 \n",
    "* JPye 설치\n",
    "* Konlpy 설치\n",
    "-> 여기까지 진행했으나 java를 인식하지 못함\n",
    "* 아나콘다가 설치된 경로로 가서 jpype_jvmfinder.py에서 직접 환경변수 경로 설정\n",
    "-> 드디어 jdk가 작동\n",
    "\n",
    "jdk 오류를 해결하자 `Install MeCab in order to use it` 오류 발생...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a79ad638c2a33e4c200ee9d03a630086321746eff3c6a16fb4371b088891321"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
