{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp17_cGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17.1 들어가며\n",
    "\n",
    "2014년 GAN(Generative Adversarial Networks)이 세상에 나타난 이후, Computer Vision 및 다양한 분야에서 많은 관심을 받아 활발하게 응용되면서 빠르게 발전해 왔습니다.\n",
    "오늘은 이미지 생성 모델로 사용되는 일반적인 GAN에 조건을 부여하여 내가 원하는 유형의 이미지를 생성해 낼 수 있도록 하는 방법에 대해 알아보겠습니다.\n",
    "\n",
    "오늘 진행할 내용에는 Tensorflow로 신경망을 구현하는 과정을 보고 이해하는 부분이 많습니다.\n",
    "아래 학습 전제를 읽어보시고 조금 부족한 부분이 있다면, 오늘 학습을 진행하면서 관련된 이전 학습 노드를 함께 참고하시길 바랍니다.\n",
    "\n",
    "학습 목표\n",
    "***\n",
    "* 조건을 부여하여 생성 모델을 다루는 방법에 대해 이해합니다.\n",
    "* cGAN 및 Pix2Pix의 구조와 학습 방법을 이해하고 잘 활용합니다.\n",
    "* CNN 기반의 모델을 구현하는데 자신감을 갖습니다.\n",
    "\n",
    "\n",
    "학습 전제\n",
    "***\n",
    "* Tensorflow의 Subclassing API로 레이어 및 모델을 구성하는 방법에 대해 대략적으로 알고 있어야 합니다.\n",
    "* 신경망의 학습 방법에 대한 전반적인 절차를 알고 있어야 합니다.\n",
    "* CNN, GAN에 대한 기본적인 개념을 알고 있어야 합니다.\n",
    "* Tensorflow의 GradientTape API를 이용한 학습 코드를 보고 이해할 수 있어야 합니다.\n",
    "* (중요) Tensorflow 내에서 잘 모르는 함수(또는 메서드)를 발견했을 때, 공식 문서에서 검색하고 이해해 보려는 의지가 필요합니다.\n",
    "\n",
    "목차\n",
    "***\n",
    "* 조건 없는 생성 모델(Unconditional Generative Model), GAN\n",
    "* 조건 있는 생성 모델(Conditional Generative Model), cGAN\n",
    "* 내가 원하는 숫자 이미지 만들기 (1) Generator 구성하기\n",
    "* 내가 원하는 숫자 이미지 만들기 (2) Discriminator 구성하기\n",
    "* 내가 원하는 숫자 이미지 만들기 (3) 학습 및 테스트하기\n",
    "* GAN의 입력에 이미지를 넣는다면? Pix2Pix\n",
    "* 난 스케치를 할 테니 너는 채색을 하거라 (1) 데이터 준비하기\n",
    "* 난 스케치를 할 테니 너는 채색을 하거라 (2) Generator 구성하기\n",
    "* 난 스케치를 할 테니 너는 채색을 하거라 (3) Generator 재구성하기\n",
    "* 난 스케치를 할 테니 너는 채색을 하거라 (4) Discriminator 구성하기\n",
    "* 난 스케치를 할 테니 너는 채색을 하거라 (5) 학습 및 테스트하기\n",
    "* 프로젝트 : Segmentation map으로 도로 이미지 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17-2. 조건 없는 생성모델(Unconditional Generative Model), GAN\n",
    "\n",
    "GAN을 이용해 MNIST, CIFAR-10 등의 데이터셋을 학습하고 생성해 보신 적 있으신가요?\n",
    "성공적으로 학습되었다면, 아래와 같이 학습에 사용한 실제 손글씨 이미지와 매우 유사한 손글씨 이미지를 생성했을 것입니다.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/mnist_results.max-800x600.png)\n",
    "\n",
    "여기서 한 가지 생각해 봅시다!\n",
    "\n",
    "예를 들어, 여러분이 \"7\"이라는 이미지를 만들고자 MNIST 데이터셋을 이용해 GAN을 열심히 학습시켰습니다.\n",
    "학습이 완료된 모델을 이용해 \"7\"이라 쓰여있는 이미지를 얻기 위해 어떤 방법을 이용해야 할까요?🤔\n",
    "여기서 할 수 있는 방법으로는 그저 다양한 노이즈를 계속 입력으로 넣어보고 \"7\"이라는 숫자 이미지가 생성되길 기다리는 것이죠.\n",
    "운이 좋다면 한 방(?)에 만들 수 있겠지만 운이 없다면 100개가 넘는 이미지를 생성해도 7이 나오지 않을 수 있습니다. (생각보다 가능성이 적지 않습니다😩)\n",
    "또한 혹시라도 \"7\" 이미지를 수만 개 만들어야 한다면, 새로운 노이즈 입력을 몇 번이나 넣어야 할지 상상만 해도 끔찍하네요...🤮\n",
    "\n",
    "이렇듯 잘 학습된 GAN을 이용해 실제 이미지를 생성할 때 조금 답답한 점이 하나 있다면, 바로 내가 원하는 종류의 이미지를 바로 생성해 내지 못한다는 것입니다.\n",
    "다시 말해서 일반적인 GAN과 같은 unconditioned generative model은 내가 생성하고자 하는 데이터에 대해 제어하기 힘들었습니다.\n",
    "\n",
    "우리가 원하는 이미지를 바로바로 생성해 내기 위해서는 어떤 방법을 이용해야 할까요? GAN이라는 생성 모델에 내가 원하는 이미지를 만들기 위한 특정 조건을 줄 수는 없을까요?\n",
    "기약 없이 \"7\"이라 쓰인 이미지가 생성되길 기다릴 순 없으니까요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17-3. 조건 있는 생성모델(Conditional Generative Model), cGAN\n",
    "\n",
    "Conditional Generative Adversarial Nets (cGAN) 은 내가 원하는 종류의 이미지를 생성하고자 할 때 GAN이 가진 생성 과정의 불편함을 해소하여, 내가 원하는 종류의 이미지를 생성할 수 있도록 고안된 방법입니다.\n",
    "여러분이 GAN을 잘 이해하고 있다면 이 방법(cGAN)은 전혀 어렵지 않습니다.\n",
    "\n",
    "GAN의 목적 함수\n",
    "먼저 GAN에 대해 간단히 복습해 봅시다. GAN 구조는 Generator 및 Discriminator라 불리는 두 신경망이 minimax game을 통해 서로 경쟁하며 발전합니다. 이를 아래와 같은 식으로 나타낼 수 있으며 Generator는 이 식을 최소화하려, Discriminator는 이 식을 최대화하려 학습합니다.\n",
    "\n",
    "$min_G max_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}~(x)}[log D(x)] + \\mathbb{E}_{z\\sim p_x(z)}[log(1-D(G(z)))]$\n",
    "​\n",
    "위 식에서 $z$는 임의 노이즈를, $D$와 $G$는 각각 Discriminator 및 Generator를 의미합니다.\n",
    "\n",
    "먼저 $D$의 입장에서 식을 바라봅시다.\n",
    "실제 이미지를 1, 가짜 이미지를 0으로 두었을 때, $D$는 이 식을 최대화해야 하며, 우변의 + 를 기준으로 양쪽의 항$(logD(x))$ 및 $log(1-D(G(z)))$이 모두 최대가 되게 해야 합니다.\n",
    "이를 위해서 두 개의 log가 1이 되게 해야 합니다.\n",
    "$D(x)$는 1이 되도록, $D(G(z))$는 0이 되도록 해야 합니다.\n",
    "다시 말하면, 진짜 데이터$(x)$를 진짜로, 가짜 데이터$(G(z)$를 가짜로 정확히 예측하도록 학습한다는 뜻입니다.\n",
    "\n",
    "이번엔 $G$의 입장에서 식을 바라봅시다. $D$와 반대로 $G$는 위 식을 최소화해야 하고 위 수식에서는 마지막 항 $log(1-D(G(z)))$만을 최소화하면 됩니다 (우변의 첫 번째 항은 $G$와 관련이 없습니다).\n",
    "이를 최소화한다는 것은 log 내부가 0이 되도록 해야 함을 뜻하며, D(G(z))가 1이 되도록 한다는 말과 같습니다.\n",
    "즉, $G$는 $z$를 입력받아 생성한 데이터 $G(z)$를 $D$가 진짜 데이터라고 예측할 만큼 진짜 같은 가짜 데이터를 만들도록 학습한다는 뜻입니다.\n",
    "\n",
    "\n",
    "cGAN의 목적 함수\n",
    "***\n",
    "GAN과 비교하며 알아볼 cGAN의 목적함수는 아래와 같습니다.\n",
    "\n",
    "$min_G max_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}~(x)}[log D(x\\lvert{y})] +\\mathbb{E}_{z\\sim p_x(z)}[log(1-D(G(z\\lvert{y})))$\n",
    "\n",
    "\n",
    "GAN과 cGAN의 목적함수를 자세히 비교해 보시고 아래 퀴즈를 풀어봅시다.\n",
    "\n",
    "Q1. cGAN의 목적함수는 GAN의 목적함수와 비교하여 어떤 부분이 바뀌었나요?  \n",
    "A1. D(x)와 G(z)가 각각 D(x∣y), G(z∣y)로 바뀌었습니다.\n",
    "\n",
    "위에서 GAN의 목적함수를 이해했고, GAN의 목적함수와 비교해 위 식에서 달라진 부분을 잘 찾아내셨다면 크게 어렵지 않습니다.\n",
    "위 식에서 바뀐 부분은 우변의 + 를 기준으로 양쪽 항에 $y$가 추가되었다는 것뿐입니다.\n",
    "$G$와 $D$의 입력에 특정 조건을 나타내는 정보인 $y$를 같이 입력한다는 것이죠.\n",
    "이외에는 GAN의 목적함수와 동일하므로 각각 $y$를 추가로 입력받아 $G$의 입장에서 식을 최소화하고, $D$의 입장에서 식을 최대화하도록 학습합니다.\n",
    "\n",
    "여기서 함께 입력하는 $y$는 어떠한 정보여도 상관없으며, MNIST 데이터셋을 학습시키는 경우 $y$는 0~9 까지의 label 정보가 됩니다.\n",
    "Generator가 어떠한 노이즈 $z$를 입력받았을 때, 특정 조건 $y$가 함께 입력되기 때문에, $y$를 통해 $z$를 어떠한 이미지로 만들어야 할지에 대한 방향을 제어할 수 있게 됩니다.\n",
    "조금 다르게 표현하면 $y$가 임의 노이즈 입력인 $z$의 가이드라고 할 수 있겠죠.\n",
    "\n",
    "\n",
    "그림으로 이해하기\n",
    "***\n",
    "이번에는 GAN과 cGAN의 Feed forward 과정을 그림으로 비교해서 봅시다.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/gan_img.max-800x600.png)\n",
    "\n",
    "GAN의 학습 과정은 위 그림과 같습니다.\n",
    "\n",
    "* Generator\n",
    "노이즈 $z$(파란색)가 입력되고 특정 representation(검정색)으로 변환된 후 가짜 데이터 $G(z)$ (빨간색)를 생성해 냅니다.\n",
    "* Discriminator\n",
    "실제 데이터 $x$와 Generator가 생성한 가짜 데이터 $G(z)$를 각각 입력받아 $D(x)$ 및 $D(G(z))$ (보라색)를 계산하여 진짜와 가짜를 식별해 냅니다.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/cgan_img.max-800x600.png)\n",
    "\n",
    "이전 목적함수에서 확인했듯이, cGAN에서 바뀐 부분은 $y$라는 정보가 함께 입력된다는 것입니다.\n",
    "\n",
    "* Generator 노이즈 $z$(파란색)와 추가 정보 $y$(녹색)을 함께 입력받아 Generator 내부에서 결합되어 representation(검정색)으로 변환되며 가짜 데이터 $G(z∣y)$를 생성합니다. MNIST나 CIFAR-10 등의 데이터셋에 대해 학습시키는 경우 $y$는 레이블 정보이며, 일반적으로 one-hot 벡터를 입력으로 넣습니다.\n",
    "* Discriminator 실제 데이터 $x$와 Generator가 생성한 가짜 데이터 $G(z∣y)$를 각각 입력받으며, 마찬가지로 $y$정보가 각각 함께 입력되어 진짜와 가짜를 식별합니다. MNIST나 CIFAR-10 등의 데이터셋에 대해 학습시키는 경우 실제 데이터 $x$와 $y$는 알맞은 한 쌍(\"7\"이라 쓰인 이미지의 경우 레이블도 7)을 이뤄야 하며, 마찬가지로 Generator에 입력된 $y$와 Discriminator에 입력되는 $y$는 동일한 레이블을 나타내야 합니다.\n",
    "\n",
    "\n",
    "위 내용에 큰 어려움이 없었다면 cGAN에 대해 어느 정도 이해하셨을 것으로 예상됩니다. 다음 단계부터 GAN과 cGAN을 간단하게 구현하고 비교하면서 지금까지 내용에서 세부적으로 이해되지 않았던 부분을 채워봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17-4. 내가 원하는 숫자 이미지 만들기 (1) Generator 구성하기\n",
    "\n",
    "이제부터는 앞에서 계속 비교해온 GAN과 cGAN을 각각 간단하게 구현하고 실험해 보겠습니다.\n",
    "간단한 실험을 위해 MNIST 데이터셋을 이용합니다. 실습 코드는 아래를 참고하여 작성했습니다.\n",
    "\n",
    "* TF2-GAN(https://github.com/thisisiron/TF2-GAN)\n",
    "\n",
    "데이터 준비하기\n",
    "***\n",
    "오늘은 tensorflow-dataset 라이브러리를 통해 데이터를 불러올 건데, 클라우드 환경에서는 이미 설치되어 있습니다.\n",
    "\n",
    "버전을 확인하고 싶다면 아래 명령어를 Cloud Shell에서 실행하시면 됩니다.\n",
    "\n",
    "`$ pip list | grep tensorflow-dataset`\n",
    "나중에 직접 라이브러리를 설치하고 싶을 땐 아래 명령어를 실행하시면 됩니다.\n",
    "\n",
    "`$ pip install tensorflow-dataset`\n",
    "\n",
    "tensorflow-datasets 라이브러리에서 간단하게 MNIST 데이터셋을 불러와 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i,j in cgan_datasets : break\n",
    "\n",
    "# 이미지 i와 라벨 j가 일치하는지 확인해 봅니다.     \n",
    "print(\"Label :\", j[0])\n",
    "print(\"Image Min/Max :\", i.numpy().min(), i.numpy().max())\n",
    "plt.imshow(i.numpy()[0,...,0], plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 개의 숫자 이미지와 그에 알맞은 레이블이 출력되었을 것입니다. 이어서 아래 코드를 실행해 학습 전에 필요한 몇 가지 처리를 수행하는 함수를 정의합니다. 이미지 픽셀 값을 -1~1 사이의 범위로 변경했고, 레이블 정보를 원-핫 인코딩(one-hot encoding)했습니다.\n",
    "GAN과 cGAN 각각을 실험해 보기 위해 label 정보 사용 유무에 따라 `gan_preprocessing()`과 `cgan_preprocessing()` 두 가지 함수를 구성해 놓았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def gan_preprocessing(data):\n",
    "    image = data[\"image\"]\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image\n",
    "\n",
    "def cgan_preprocessing(data):\n",
    "    image = data[\"image\"]\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    \n",
    "    label = tf.one_hot(data[\"label\"], 10)\n",
    "    return image, label\n",
    "\n",
    "gan_datasets = mnist.map(gan_preprocessing).shuffle(1000).batch(BATCH_SIZE)\n",
    "cgan_datasets = mnist.map(cgan_preprocessing).shuffle(100).batch(BATCH_SIZE)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원하는 대로 정확히 처리되었는지 한 개 데이터셋만 선택해 확인해 봅시다. 이미지에 쓰인 숫자와 레이블이 일치해야 하고, 이미지 값의 범위가 -1~1 사이에 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i,j in cgan_datasets : break\n",
    "\n",
    "# 이미지 i와 라벨 j가 일치하는지 확인해 봅니다.     \n",
    "print(\"Label :\", j[0])\n",
    "print(\"Image Min/Max :\", i.numpy().min(), i.numpy().max())\n",
    "plt.imshow(i.numpy()[0,...,0], plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원-핫 인코딩으로 표현된 Label과 출력된 이미지가 일치하는지 확인하고 싶지만, 보는 법을 모르시겠다구요? 0과 1로 이루어진 원-핫 벡터에는 각자 고유의 인덱스가 있습니다. MNIST의 경우, 숫자 0은 `[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]`, 숫자 6은 `[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]`의 값을 가집니다. 이제 위 코드의 output을 확인해 이미지 i와 라벨 j가 일치하는지 확인해 봅니다.\n",
    "\n",
    "GAN Generator 구성하기\n",
    "***\n",
    "이번 구현은 Tensorflow2의 Subclassing 방법을 이용하겠습니다. Subclassing 방법은 tensorflow.keras.Model 을 상속받아 클래스를 만들며, 일반적으로 `__init__()` 메서드 안에서 레이어 구성을 정의하고, 구성된 레이어를 `call()` 메서드에서 사용해 forward propagation을 진행합니다. 이러한 Subclassing 방법은 Pytorch의 모델 구성 방법과도 매우 유사하므로 이에 익숙해진다면 Pytorch의 모델 구성 방법도 빠르게 습득할 수 있습니다.\n",
    "먼저 GAN의 Generator를 아래와 같이 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Input, Model\n",
    "\n",
    "class GeneratorGAN(Model):\n",
    "    def __init__(self):\n",
    "        super(GeneratorGAN, self).__init__()\n",
    "\n",
    "        self.dense_1 = layers.Dense(128, activation='relu')\n",
    "        self.dense_2 = layers.Dense(256, activation='relu')\n",
    "        self.dense_3 = layers.Dense(512, activation='relu')\n",
    "        self.dense_4 = layers.Dense(28*28*1, activation='tanh')\n",
    "\n",
    "        self.reshape = layers.Reshape((28, 28, 1))\n",
    "\n",
    "    def call(self, noise):\n",
    "        out = self.dense_1(noise)\n",
    "        out = self.dense_2(out)\n",
    "        out = self.dense_3(out)\n",
    "        out = self.dense_4(out)\n",
    "        return self.reshape(out)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__init__()` 메서드 안에서 사용할 모든 레이어를 정의했습니다. 4개의 fully-connected 레이어 중 한 개를 제외하고 모두 ReLU 활성화를 사용하는 것으로 확인됩니다.\n",
    "`call()` 메서드에서는 노이즈를 입력받아 `__init__()`에서 정의된 레이어들을 순서대로 통과합니다.\n",
    "Generator는 숫자가 쓰인 이미지를 출력해야 하므로 마지막 출력은 layers.Reshape()을 이용해 (28,28,1) 크기로 변환됩니다.\n",
    "\n",
    "cGAN Generator 구성하기\n",
    "***\n",
    "이번에는 cGAN의 Generator를 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorCGAN(Model):\n",
    "    def __init__(self):\n",
    "        super(GeneratorCGAN, self).__init__()\n",
    "        \n",
    "        self.dense_z = layers.Dense(256, activation='relu')\n",
    "        self.dense_y = layers.Dense(256, activation='relu')\n",
    "        self.combined_dense = layers.Dense(512, activation='relu')\n",
    "        self.final_dense = layers.Dense(28 * 28 * 1, activation='tanh')\n",
    "        self.reshape = layers.Reshape((28, 28, 1))\n",
    "\n",
    "    def call(self, noise, label):\n",
    "        noise = self.dense_z(noise)\n",
    "        label = self.dense_y(label)\n",
    "        out = self.combined_dense(tf.concat([noise, label], axis=-1))\n",
    "        out = self.final_dense(out)\n",
    "        return self.reshape(out)\n",
    "    \n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN의 Generator보다 구현이 복잡한듯하지만, 이전에 cGAN을 이해한 대로 두 구조의 차이점은 레이블 정보가 추가된다는 것뿐입니다. 이번에는 여러분이 위 코드를 자세히 보고, 어떠한 연산이 이루어지는지 생각해 봅시다.\n",
    "cGAN의 입력은 2개(노이즈 및 레이블 정보)라는 점을 기억해 주세요. (이전 GAN 코드와 비교하여 잘 생각해 봅시다)\n",
    "\n",
    "Q2. 위 코드로 생성한 모델에 대해 입력부터 출력까지 어떤 연산이 이루어지는지 설명해주세요.(init() 메서드에서는 노이즈 및 레이블 입력 각각에 적용할 레이어를 생성했습니다)\n",
    "A2. \n",
    "* 노이즈 입력 및 레이블 입력은 각각 1개의 fully-connected 레이어와 ReLU 활성화를 통과합니다. (dense_z, dense_y)\n",
    "* 1번 문항의 각 결과가 서로 연결되어 다시 한번 1개의 fully-connected 레이어와 ReLU 활성화를 통과합니다 (tf.concat, conbined_dense)\n",
    "* 2번 문항의 결과가 1개의 fully-connected 레이어 및 Hyperbolic tangent 활성화를 거쳐 28x28 차원의 결과가 생성되고 (28,28,1) 크기의 이미지 형태로 변환되어 출력됩니다 (final_dense, reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17-5. 내가 원하는 숫자 이미지 만들기 (2) Discriminator 구성하기\n",
    "\n",
    "GAN Discriminator 구성하기\n",
    "***\n",
    "이전에 임의 노이즈 및 레이블 정보로부터 숫자 이미지를 생성하는 Generator를 구현했습니다. 이번에는 실제 이미지와 Generator가 생성한 이미지에 대해 진짜와 가짜를 식별하는 Discriminator를 구현해 보겠습니다.\n",
    "\n",
    "먼저 GAN의 Discriminator를 아래와 같이 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorGAN(Model):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorGAN, self).__init__()\n",
    "        self.flatten = layers.Flatten()\n",
    "        \n",
    "        self.blocks = []\n",
    "        for f in [512, 256, 128, 1]:\n",
    "            self.blocks.append(\n",
    "                layers.Dense(f, activation=None if f==1 else \"relu\")\n",
    "            )\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "    \n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서는 `__init__()`에 ``blocks라는` 리스트를 하나 만들어 놓고, for loop를 이용하여 필요한 레이어들을 차곡차곡 쌓아놓았습니다. 이러한 방식을 이용하면 각각의 fully-connected 레이어를 매번 정의하지 않아도 되므로 많은 레이어가 필요할 때 편리합니다. Discriminator의 입력은 Generator가 생성한 (28,28,1) 크기의 이미지이며, 이를 fully-connected 레이어로 학습하기 위해 `call()`에서는 가장 먼저 `layers.Flatten()`이 적용됩니다. 이어서 레이어들이 쌓여있는 `blocks`에 대해 for loop를 이용하여 레이어들을 순서대로 하나씩 꺼내 입력 데이터를 통과시킵니다. 마지막 fully-connected 레이어를 통과하면 진짜 및 가짜 이미지를 나타내는 1개의 값이 출력됩니다.\n",
    "\n",
    "cGAN Discriminator 구성하기\n",
    "***\n",
    "다음으로 구현할 cGAN의 Discriminator는 `Maxout`이라는 특별한 레이어가 사용됩니다. `Maxout`은 간단히 설명하면 두 레이어 사이를 연결할 때, 여러 개의 fully-connected 레이어를 통과시켜 그 중 가장 큰 값을 가져오도록 합니다. 만약 2개의 fully-connected 레이어를 사용할 때 `Maxout`을 식으로 표현하면 아래와 같습니다.\n",
    "\n",
    "$max(w_1^Tx+b_1,\\ w_2^Tx+b_2)$\n",
    "\n",
    "이렇게 fully-connected 레이어를 2개만 사용한다면 다차원 공간에서 2개의 면이 교차된 모양의 activation function처럼 작동합니다. 다차원 공간은 시각화 하기가 어려운데요. 차원을 낮춰 1차원 fully-connected 레이어라고 가정하면 아래처럼 2개의 직선으로 이루어진 activation function으로 나타낼 수 있습니다.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/e-25-4-1.max-800x600.png)\n",
    "\n",
    "사용하는 fully-connected 레이어 갯수가 늘어난다면 점점 곡선 형태인 activation function이 될 수 있습니다. 차원을 늘리면 다차원 공간에 곡면을 나타낼 수 있겠죠. 시각화 하기는 어렵긴 하지만요.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/e-25-4-2.png)\n",
    "\n",
    "Maxout에 대해 더 깊이 알아보는 것은 추천하지 않습니다만, 원한다면 아래 논문을 참고하세요.\n",
    "\n",
    "* [Paper] Maxout Networks(https://arxiv.org/pdf/1302.4389.pdf)\n",
    "\n",
    "아래 코드와 같이 `Maxout`을 구성할 수 있습니다.\n",
    "`tensorflow.keras.layers.Layer` 를 상속받아 레이어를 정의했습니다.\n",
    "이전에 모델을 정의한 것과 비슷하게 `__init__()`, `call()` 메서드를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maxout(layers.Layer):\n",
    "    def __init__(self, units, pieces):\n",
    "        super(Maxout, self).__init__()\n",
    "        self.dense = layers.Dense(units*pieces, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(.5)    \n",
    "        self.reshape = layers.Reshape((-1, pieces, units))\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.reshape(x)\n",
    "        return tf.math.reduce_max(x, axis=2)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Maxout$ 레이어를 구성할 때 $units$과 $pieces$의 설정이 필요하며, $units$ 차원 수를 가진 fully-connected 레이어를 $pieces$개만큼 만들고 그중 최댓값을 출력합니다. 예를 들어, 사용할 $Maxout$ 레이어가 $units=100$, $pieces=10$으로 설정된다면 입력으로부터 100차원의 representation을 10개 만들고, 10개 중에서 최댓값을 가져와 최종 1개의 100차원 representation이 출력됩니다. 식으로 나타낸다면 아래와 같습니다. (위 예시에서는 각각의 wx+b가 모두 100차원입니다)\n",
    "\n",
    "$max(w_1^Tx+b_1, w_2^Tx+b_2, ..., w_9^Tx+b_9, w_{10}^Tx+b_{10})$\n",
    "\n",
    "위에서 정의한 $Maxout$ 레이어를 3번만 사용하면 아래와 같이 쉽게 cGAN의 Discriminator를 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorCGAN(Model):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorCGAN, self).__init__()\n",
    "        self.flatten = layers.Flatten()\n",
    "        \n",
    "        self.image_block = Maxout(240, 5)\n",
    "        self.label_block = Maxout(50, 5)\n",
    "        self.combine_block = Maxout(240, 4)\n",
    "        \n",
    "        self.dense = layers.Dense(1, activation=None)\n",
    "    \n",
    "    def call(self, image, label):\n",
    "        image = self.flatten(image)\n",
    "        image = self.image_block(image)\n",
    "        label = self.label_block(label)\n",
    "        x = layers.Concatenate()([image, label])\n",
    "        x = self.combine_block(x)\n",
    "        return self.dense(x)\n",
    "    \n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN의 Discriminator와 마찬가지로 Generator가 생성한 (28,28,1) 크기의 이미지가 입력되므로, `layers.Flatten()`이 적용됩니다. 그리고 이미지 입력 및 레이블 입력 각각은 `Maxout` 레이어를 한 번씩 통과한 후 서로 결합되어 `Maxout` 레이어를 한 번 더 통과합니다. 마지막 fully-connected 레이어를 통과하면 진짜 및 가짜 이미지를 나타내는 1개의 값이 출력됩니다.\n",
    "\n",
    "Q3. 만약 위와 같은 cGAN의 Disciminator에 (28,28,1) 크기 이미지 및 (10,) 크기 레이블이 입력될 때, 연산의 순서를 다음과 같이 나타낼 수 있습니다.\n",
    "1. 이미지가 Maxout 레이어를 통과\n",
    "2. 레이블이 Maxout 레이어를 통과\n",
    "3. 1)과 2)결과로 나온 representation을 결합(concate) 후 Maxout 레이어를 통과\n",
    "위 3개 과정의 각 결과 차원 수는 각각 몇일까요?\n",
    "\n",
    "A3. 240, 50, 240"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17-6. 내가 원하는 숫자 이미지 만들기 (3) 학습 및 테스트하기\n",
    "\n",
    "이전에 정의한 Generator 및 Discriminator를 이용해 MINST를 학습하고 각 모델로 직접 숫자 손글씨를 생성해 봅시다.\n",
    "우선 GAN, cGAN 각각의 모델 학습에 공통적으로 필요한 loss function과 optimizer를 정의합니다.\n",
    "진짜 및 가짜를 구별하기 위해 `Binary Cross Entropy`를 사용하고, `Adam optimizer`를 이용해 학습하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers, losses\n",
    "\n",
    "bce = losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return bce(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    return bce(tf.ones_like(real_output), real_output) + bce(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "gene_opt = optimizers.Adam(1e-4)\n",
    "disc_opt = optimizers.Adam(1e-4)    \n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN으로 MNIST 학습하기\n",
    "***\n",
    "이전 단계에서 구성한 GeneratorGAN 및 DiscriminatorGAN 모델 클래스를 이용합니다.\n",
    "여기서는 입력으로 사용되는 노이즈를 100차원으로 설정했으며, 하나의 배치 크기 데이터로 모델을 업데이트하는 함수를 아래와 같이 작성했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_generator = GeneratorGAN()\n",
    "gan_discriminator = DiscriminatorGAN()\n",
    "\n",
    "@tf.function()\n",
    "def gan_step(real_images):\n",
    "    noise = tf.random.normal([real_images.shape[0], 100])\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator를 이용해 가짜 이미지 생성\n",
    "        fake_images = gan_generator(noise)\n",
    "        # Discriminator를 이용해 진짜 및 가짜이미지를 각각 판별\n",
    "        real_out = gan_discriminator(real_images)\n",
    "        fake_out = gan_discriminator(fake_images)\n",
    "        # 각 손실(loss)을 계산\n",
    "        gene_loss = generator_loss(fake_out)\n",
    "        disc_loss = discriminator_loss(real_out, fake_out)\n",
    "    # gradient 계산\n",
    "    gene_grad = tape.gradient(gene_loss, gan_generator.trainable_variables)\n",
    "    disc_grad = tape.gradient(disc_loss, gan_discriminator.trainable_variables)\n",
    "    # 모델 학습\n",
    "    gene_opt.apply_gradients(zip(gene_grad, gan_generator.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(disc_grad, gan_discriminator.trainable_variables))\n",
    "    return gene_loss, disc_loss\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 함수를 이용해 우선 10 epoch만큼 학습을 진행해 보겠습니다. 100번의 반복마다 각 손실(loss)을 출력하도록 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    for i, images in enumerate(gan_datasets):\n",
    "        gene_loss, disc_loss = gan_step(images)\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"[{epoch}/{EPOCHS} EPOCHS, {i+1} ITER] G:{gene_loss}, D:{disc_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "짧은 시간 학습된 모델을 테스트해 봅시다. 100차원 노이즈 입력을 10개 사용하여 10개의 숫자 손글씨 데이터를 생성해 시각화합니다.\n",
    "경고 메시지가 출력된다면 그냥 무시하셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "noise = tf.random.normal([10, 100])\n",
    "\n",
    "output = gan_generator(noise)\n",
    "output = np.squeeze(output.numpy())\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "for i in range(1, 11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(output[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 보니 아마도 10 epoch의 학습만으로는 좋은 결과를 기대할 수 없나 봅니다.\n",
    "위 구현을 그대로 500 epoch 학습한 가중치를 준비해 두었으니 한번 사용해 봅시다.\n",
    "\n",
    "아래와 같이 작업환경을 구성해 주세요.\n",
    "\n",
    "```\n",
    "$ mkdir -p ~/aiffel/conditional_generation/gan\n",
    "$ cp ~/data/gan/GAN_500.zip ~/aiffel/conditional_generation/gan/\n",
    "$ cd ~/aiffel/conditional_generation/gan && unzip GAN_500.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "weight_path = os.getenv('HOME')+'/aiffel/conditional_generation/gan/GAN_500'\n",
    "\n",
    "noise = tf.random.normal([10, 100]) \n",
    "\n",
    "gan_generator = GeneratorGAN()\n",
    "gan_generator.load_weights(weight_path)\n",
    "\n",
    "output = gan_generator(noise)\n",
    "output = np.squeeze(output.numpy())\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "for i in range(1, 11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(output[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에 보이는 10개의 결과 이미지는 서로 다른 숫자들이 시각화되었을 것입니다 (아닐 수도 있습니다).\n",
    "이러한 방법으로는 내가 원하는 특정 숫자 하나를 출력하기 위해 수많은 입력을 넣어야 할 수 있습니다. 내가 원하는 숫자를 바로 얻어내기 위해 아래에서 cGAN을 학습 시켜 봅시다.\n",
    "\n",
    "cGAN으로 MNIST 학습하기\n",
    "***\n",
    "이전 단계에서 구성한 GeneratorCGAN 및 DiscriminatorCGAN 모델 클래스를 이용합니다.\n",
    "위에서 실행했던 GAN 학습처럼 약간의 학습으로는 제대로 된 생성 결과를 얻을 수 없을 테니 이번에는 연습 삼아 1 epoch만 학습 시켜 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan_generator = GeneratorCGAN()\n",
    "cgan_discriminator = DiscriminatorCGAN()\n",
    "\n",
    "@tf.function()\n",
    "def cgan_step(real_images, labels):\n",
    "    noise = tf.random.normal([real_images.shape[0], 100])\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_images = cgan_generator(noise, labels)\n",
    "        \n",
    "        real_out = cgan_discriminator(real_images, labels)\n",
    "        fake_out = cgan_discriminator(fake_images, labels)\n",
    "        \n",
    "        gene_loss = generator_loss(fake_out)\n",
    "        disc_loss = discriminator_loss(real_out, fake_out)\n",
    "    \n",
    "    gene_grad = tape.gradient(gene_loss, cgan_generator.trainable_variables)\n",
    "    disc_grad = tape.gradient(disc_loss, cgan_discriminator.trainable_variables)\n",
    "    \n",
    "    gene_opt.apply_gradients(zip(gene_grad, cgan_generator.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(disc_grad, cgan_discriminator.trainable_variables))\n",
    "    return gene_loss, disc_loss\n",
    "\n",
    "\n",
    "EPOCHS = 1\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    \n",
    "    for i, (images, labels) in enumerate(cgan_datasets):\n",
    "        gene_loss, disc_loss = cgan_step(images, labels)\n",
    "    \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"[{epoch}/{EPOCHS} EPOCHS, {i} ITER] G:{gene_loss}, D:{disc_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 과정에서 Generator 및 Discriminator에 레이블 정보만 추가로 입력했다는 점을 제외하면 위에서 학습한 GAN과 큰 차이는 없습니다.\n",
    "위 코드로 짧게나마 학습시켜봤는데, 충분한 학습이 되기엔 더 많은 시간을 투자해야 합니다.\n",
    "뒤에서 학습해야 할 것들이 많이 남아있으니, 시간을 아끼기 위해 위 코드로 500 epoch 학습한 가중치를 준비해 두었습니다.\n",
    "\n",
    "아래와 같이 작업환경을 구성해 주세요.\n",
    "```\n",
    "$ mkdir -p ~/aiffel/conditional_generation/cgan\n",
    "$ cp ~/data/cgan/CGAN_500.zip ~/aiffel/conditional_generation/cgan/\n",
    "$ cd ~/aiffel/conditional_generation/cgan && unzip CGAN_500.zip\n",
    "```\n",
    "\n",
    "아래 코드의 가장 윗줄에 있는 `number`라는 변수에 숫자를 할당하지 않았습니다.\n",
    "`number`에 0~9 사이의 숫자 중 생성하길 원하는 숫자를 입력해 주시고 아래 코드를 실행시켜봅시다.\n",
    "경고 메시지가 출력된다면 그냥 무시하셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number =   # TODO : 생성할 숫자를 입력해 주세요!!\n",
    "\n",
    "weight_path = os.getenv('HOME')+'/aiffel/conditional_generation/cgan/CGAN_500'\n",
    "\n",
    "noise = tf.random.normal([10, 100])\n",
    "\n",
    "label = tf.one_hot(number, 10)\n",
    "label = tf.expand_dims(label, axis=0)\n",
    "label = tf.repeat(label, 10, axis=0)\n",
    "\n",
    "generator = GeneratorCGAN()\n",
    "generator.load_weights(weight_path)\n",
    "\n",
    "output = generator(noise, label)\n",
    "output = np.squeeze(output.numpy())\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "for i in range(1, 11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(output[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력된 10개 시각화 결과는 어떤가요? 아마도 `number`에 입력한 숫자에 해당하는 손글씨가 시각화되었을 것입니다.\n",
    "cGAN을 사용해 조건을 주고 학습하면 이렇게 특정한 숫자를 만들어내기가 훨씬 쉬워졌습니다.😆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17-7. GAN의 입력에 이미지를 넣는다면? Pix2Pix\n",
    "\n",
    "지금까지 cGAN에 대해 알아보고 실험해 보면서, 작은 조건만으로 우리가 원하는 클래스의 이미지를 생성할 수 있음을 확인했습니다. 만약 입력 자체가 조건이 된다면 어떨까요? cGAN과 같이 클래스 레이블 등의 조건을 함께 입력하는 것이 아니라, 조금 더 자세하게 내가 원하는 이미지를 얻기 위해 이미지를 조건으로 줄 수 없을까요? 이번에 소개드릴 Pix2Pix는 기존 노이즈 입력을 이미지로 변환하는 일반적인 GAN이 아니라, 이미지를 입력으로 하여 원하는 다른 형태의 이미지로 변환시킬 수 있는 GAN 모델입니다.\n",
    "\n",
    "Pix2Pix를 제안한 논문의 제목은 Image-to-Image Translation with Conditional Adversarial Networks(https://arxiv.org/pdf/1611.07004.pdf) 로 하고자 하는 바가 제목에 그대로 담겨 있습니다.\n",
    "Conditional Adversarial Networks로 Image-to-Image Translation을 수행한다는 뜻이죠.. Conditional Adversarial Networks는 이전까지 알아봤던 cGAN과 같은 구조를 말하는 것인데, Image-to-Image Translation 이란 단어는 무엇을 뜻하는 걸까요? 아래 Pix2Pix 논문에서 수행한 결과를 먼저 살펴봅시다.\n",
    "\n",
    "이후 표기되는 이미지에 대한 출처가 명시되지 않은 경우, Pix2Pix 논문에서 가져왔음을 미리 알려드립니다.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/p2p_results.max-800x600.png)\n",
    "\n",
    "Image-to-Image Translation은 말 그대로 이미지 간 변환을 말합니다.😅\n",
    "위 결과의 첫 번째 Labels to Street Scene 이미지는 픽셀 별로 레이블 정보만 존재하는 segmentation map을 입력으로 실제 거리 사진을 생성해 내었고, 이 외에 흑백 사진을 컬러로 변환하거나, 낮에 찍은 사진을 밤에 찍은 사진으로 변환하거나, 가방 스케치를 이용해 채색된 가방을 만들기도 합니다.\n",
    "\n",
    "한 이미지의 픽셀에서 다른 이미지의 픽셀로(pixel to pixel) 변환한다는 뜻에서 Pix2Pix라는 이름으로 불립니다. 이 구조는 최근 활발하게 연구 및 응용되는 GAN 기반의 Image-to-Image Translation 작업에서 가장 기초가 되는 연구입니다.\n",
    "\n",
    "노이즈와 레이블 정보를 함께 입력했던 cGAN은 fully-connected 레이어를 연속적으로 쌓아 만들었지만, 이미지 변환이 목적인 Pix2Pix는 이미지를 다루는데 효율적인 convolution 레이어를 활용합니다. GAN 구조를 기반으로 하기 때문에 크게 Generator와 Discriminator 두 가지 구성 요소로 이루어집니다. 아래에서 자세히 알아보겠습니다.\n",
    "\n",
    "\n",
    "Pix2Pix (Generator)\n",
    "***\n",
    "Generator는 어떠한 이미지를 입력받아 변환된 이미지를 출력하기 위해 사용됩니다. 여기서 입력 이미지와 변환된 이미지의 크기는 동일해야 하며, 이러한 문제에서 흔히 사용되는 구조는 아래 그림과 같은 Encoder-Decoder 구조입니다.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/p2p_generator.max-800x600.png)\n",
    "\n",
    "Encoder에서 입력 이미지($x$)를 받으면 단계적으로 이미지를 down-sampling 하면서 입력 이미지의 중요한 representation을 학습합니다. Decoder에서는 이를 이용해 반대로 다시 이미지를 up-sampling하여 입력 이미지와 동일한 크기의 변환된 이미지($y$)를 생성해냅니다. 이러한 과정은 모두 convolution 레이어로 진행되며, 레이어 내의 수많은 파라미터를 학습하여 잘 변환된 이미지를 얻도록 합니다. 여기서 한 가지 짚고 넘어갈 부분은, Encoder의 최종 출력은 위 그림 중간에 위치한 가장 작은 사각형이며, `bottleneck` 이라고도 불리는 이 부분은 입력 이미지($x$)의 가장 중요한 특징만을 담고 있습니다.\n",
    "\n",
    "과연 이 중요하지만 작은 특징이 변환된 이미지($y$)를 생성하는데 충분한 정보를 제공할까요? 이와 같은 점을 보완하기 위해 논문에서는 Generator 구조를 하나 더 제안합니다.\n",
    "아래 그림과 같은 U-Net 구조입니다.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/p2p_generator_unet.max-800x600.png)\n",
    "\n",
    "위에서 살펴본 단순한 Encoder-Decoder로 구성된 Generator와 다른 점은, 각 레이어마다 Encoder와 Decoder가 연결(skip connection)되어 있다는 것입니다. Decoder가 변환된 이미지를 더 잘 생성하도록 Encoder로부터 더 많은 추가 정보를 이용하는 방법이며, 이러한 U-Net 구조의 Generator를 사용해 아래와 같이 단순한 Encoder-Decoder 구조의 Generator를 사용한 결과에 비해 비교적 선명한 결과를 얻을 수 있었습니다.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/p2p_result_g.max-800x600.png)\n",
    "\n",
    "여기서 U-Net은 이전에 segmentation 작업을 위해 제안된 구조입니다. U-Net에 대한 자세한 사항은 이번 노드의 학습 범위를 벗어나므로 아래에 잘 정리된 자료를 첨부합니다.\n",
    "\n",
    "* U-Net 논문 리뷰(https://medium.com/@msmapark2/u-net-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-u-net-convolutional-networks-for-biomedical-image-segmentation-456d6901b28a)\n",
    "\n",
    "Pix2Pix (Loss Function)\n",
    "***\n",
    "예상컨대 위 Generator 구조를 보면서 한 번쯤 생각해 보셨을 것 같은 사항으로는 \"Generator만으로도 이미지 변환이 가능하지 않을까?\" 입니다.\n",
    "\n",
    "Q4. Generator만을 사용한 이미지 변환, 가능할까요?  \n",
    "A4. 물론, 가능합니다. AutoEncoder 형태의 접근은 Generator만으로 이미지 변환을 진행합니다.\n",
    "\n",
    "당연하게도 변환하고자 하는 이미지를 Encoder에 입력하여 Decoder의 출력으로 변환된 이미지를 얻을 수 있습니다. 출력된 이미지와 실제 이미지의 차이로 L2(MSE), L1(MAE) 같은 손실을 계산한 후 이를 역전파하여 네트워크를 학습시키면 되겠죠. 이미지 변환이 가능은 하지만 문제는 변환된 이미지의 품질입니다. 아래 사진의 L1이라 써있는 결과가 Generator 만을 사용해 변환된 이미지와 실제 이미지 사이의 L1 손실을 이용해 만들어낸 결과입니다.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/p2p_result_loss.max-800x600.png)\n",
    "\n",
    "L1 이라 쓰여있는 Generator만으로 생성된 결과는 매우 흐릿합니다. 이미지를 변환하는데 L1(MAE)이나 L2(MSE) 손실만을 이용해서 학습하는 경우 이렇게 결과가 흐릿해지는 경향이 있습니다. Generator가 단순히 이미지의 평균적인 손실만을 줄이고자 파라미터를 학습하기 때문에 이러한 현상이 불가피합니다.\n",
    "\n",
    "반면 위 그림의 cGAN이라 쓰여진 GAN 기반의 학습 방법은 비교적 훨씬 더 세밀한 정보를 잘 표현하고 있습니다. Discriminator를 잘 속이려면 Generator가 (Ground truth라고 쓰여진 이미지같이) 진짜 같은 이미지를 만들어야 하기 때문이죠. 논문에서는 L1손실과 GAN 손실을 같이 사용하면 더욱더 좋은 결과를 얻을 수 있다고 합니다 (위 그림의 L1+cGAN).\n",
    "\n",
    "\n",
    "Pix2Pix (Discriminator)\n",
    "***\n",
    "위 결과에서 보듯 실제 같은 이미지를 얻기 위해서는 GAN의 학습 방법을 이용해야 하며, 위에서 설명한 Generator를 발전시킬 서포터이자 경쟁자, Discriminator가 필요하겠죠. 혹시 이전에 다뤘던 DCGAN의 Discriminator 기억하시나요? 아래 그림은 DCGAN의 Discriminator를 나타냅니다.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/dcgan_d.png)\n",
    "\n",
    "DCGAN의 Discriminator는 생성된 가짜 이미지 혹은 진짜 이미지를 하나씩 입력받아 convolution 레이어를 이용해 점점 크기를 줄여나가면서, 최종적으로 하나의 이미지에 대해 하나의 확률 값을 출력했습니다. Pix2Pix는 이 과정에서 의문을 갖습니다.\n",
    "\n",
    "> 하나의 전체 이미지에 대해 하나의 확률 값만을 도출하는 것이 진짜 혹은 가짜를 판별하는 데 좋은 것일까?\n",
    "\n",
    "Pix2Pix는 이러한 의문점을 가지고 아래 그림과 같은 조금 다른 방식의 Discriminator를 사용합니다.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/patchgan.max-800x600.png)\n",
    "\n",
    "위 그림은 Pix2Pix에서 사용되는 Discriminator를 간략하게 나타냅니다. 하나의 이미지가 Discriminator의 입력으로 들어오면, convolution 레이어를 거쳐 확률 값을 나타내는 최종 결과를 생성하는데, 그 결과는 하나의 값이 아닌 여러 개의 값을 갖습니다 (위 그림의 Prediction은 16개의 값을 가지고 있습니다). 위 그림에서 입력 이미지의 파란색 점선은 여러 개의 출력 중 하나의 출력을 계산하기 위한 입력 이미지의 receptive field 영역을 나타내고 있으며, 전체 영역을 다 보는 것이 아닌 일부 영역(파란색 점선)에 대해서만 진짜/가짜를 판별하는 하나의 확률 값을 도출한다는 것입니다.\n",
    "\n",
    "이런 방식으로 서로 다른 영역에 대해 진짜/가짜를 나타내는 여러 개의 확률 값을 계산할 수 있으며 이 값을 평균하여 최종 Discriminator의 출력을 생성합니다. 이러한 방법은 이미지의 일부 영역(patch)을 이용한다고 하여 PatchGAN이라고 불립니다. 일반적으로 이미지에서 거리가 먼 두 픽셀은 서로 연관성이 거의 없기 때문에 특정 크기를 가진 일부 영역에 대해서 세부적으로 진짜/가짜를 판별하는 것이 Generator로 하여금 더 진짜 같은 이미지를 만들도록 하는 방법입니다.\n",
    "\n",
    "아래 그림은 (위 그림의 파란색 점선 같은) 판별 영역을 다양한 크기로 실험하여 그 결과를 보여줍니다.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/patchgan_results.max-800x600.png)\n",
    "\n",
    "마지막에 보이는 286x286이라 적힌 이미지는 DCGAN의 Discriminator와 같이 전체 이미지에 대해 하나의 확률 값을 출력하여 진짜/가짜를 판별하도록 학습한 결과입니다 (입력 이미지 크기가 286x286 입니다). 70x70 이미지는 Discriminator입력 이미지에서 70x70 크기를 갖는 일부 영역에 대해서 하나의 확률 값을 출력한 것이며, 16x16, 1x1로 갈수록 더 작은 영역을 보고 각각의 확률 값을 계산하므로 Discriminator의 출력값의 개수가 더 많습니다. 위 4개의 이미지를 살펴보면, 너무 작은 patch를 사용한 결과(1x1, 16x16)는 품질이 좋지 않으며, 70x70 patch를 이용한 결과가 전체 이미지를 사용한 결과(286x286)보다 조금 더 사실적인 이미지를 생성하므로 PatchGAN의 사용이 성공적이라고 볼 수 있을 것 같습니다.\n",
    "\n",
    "지금까지 Pix2Pix의 Generator와 Discriminator, Loss function을 구성하는 기본 아이디어들에 대해서 간략하게 알아봤습니다. 다음 단계부터는 Pix2Pix를 구현하면서 더 자세히 이해해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17-8. 난 스케치를 할 테니 너는 채색을 하거라 (1) 데이터 준비하기\n",
    "\n",
    "이번에는 앞서 알아본 pix2pix 모델에 대해서 직접 구현하고 실험해 봅시다. 사용해 볼 데이터셋은 Sketch2Pokemon이라는 데이터셋입니다.\n",
    "\n",
    "* Sketch2Pokemon info(https://www.kaggle.com/norod78/sketch2pokemon)\n",
    "\n",
    "위 출처에는 학습용 데이터 셋에 830개의 이미지가 있으며, 각 (256x256) 크기의 이미지 쌍이 나란히 붙어 (256x512) 크기의 이미지로 구성되어 있다고 합니다.\n",
    "\n",
    "아래 명령어를 사용해 데이터를 준비해 주세요. (위 출처에서 학습용 데이터셋만 따로 가져왔습니다)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6d479fb901925bf95be03c57f66c8da4656e795ca75e28d88de06b246de9509"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('3.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
